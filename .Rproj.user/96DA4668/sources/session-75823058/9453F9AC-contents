---
title: "Benchmarks"
author: "Nelvis Fornasin"
format: html
editor: visual
---

## About this page

This page contains benchmarks for different reserving algorithms. If you are wondering just how good your model performance is, you can download the sample datasets, run it on them and see how it fares in comparison with our models.

## Data

We take as input data 5 synthetically generated single claim development datasets of increasing complexity. The datasets are the example datasets from SPLICE, available here: https://github.com/agi-lab/SPLICE/tree/main/datasets (incurred_1.csv, incurred_2.csv, incurred_3.csv, incurred_4.csv, incurred_5.csv).

To allow for comparison with classical triangle methods we only feed the models the occurrence and development time of the single claims. For triangle methods (e.g. Chain Ladder) this data is then aggregated, while for e.g. neural networks the data is fed unaggregated into the model.

## Benchmarks

The table shows the RMSE for each model.

| Model        | Incurred 1 | Incurred 2 | Incurred 3 | Incurred 4 | Incurred 5 | Details |
|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
| Chain Ladder | 125837.8   | 134866.3   | 138628.9   | 297297.5   | 289020.2   |         |
| LSTM         |            |            |            |            |            |         |
| Boosted NN   |            |            |            |            |            |         |
| XGBoost      |            |            |            |            |            |         |

: RMSE per SPLICE dataset
